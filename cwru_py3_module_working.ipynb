{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import errno\n",
    "import random\n",
    "import urllib\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from requests import get  # to make GET request\n",
    "\n",
    "\n",
    "class CWRU:\n",
    "\n",
    "    def __init__(self, exp, rpm, length):\n",
    "        if exp not in ('12DriveEndFault', '12FanEndFault', '48DriveEndFault'):\n",
    "            print(\"wrong experiment name: {}\".format(exp))\n",
    "            exit(1)\n",
    "        if rpm not in ('1797', '1772', '1750', '1730'):\n",
    "            print(\"wrong rpm value: {}\".format(rpm))\n",
    "            exit(1)\n",
    "        # root directory of all data\n",
    "        rdir = os.path.join('DataSetCWRU',\n",
    "                            exp,\n",
    "                            rpm)\n",
    "\n",
    "        fmeta = os.path.join('metadata.txt')\n",
    "        all_lines = open(fmeta).readlines()\n",
    "        lines = []\n",
    "        for line in all_lines:\n",
    "            l = line.split()\n",
    "            if (l[0] == exp or l[0] == 'NormalBaseline') and l[1] == rpm:\n",
    "                lines.append(l)\n",
    "\n",
    "        self.length = length  # sequence length\n",
    "        self._load_and_slice_data(rdir, lines)\n",
    "        # shuffle training and test arrays\n",
    "        #self._shuffle()\n",
    "        self.labels = tuple(line[2] for line in lines)\n",
    "        self.nclasses = len(self.labels)  # number of classes\n",
    "\n",
    "    def _mkdir(self, path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError as exc:\n",
    "            if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "                pass\n",
    "            else:\n",
    "                print(\"can't create directory '{}''\".format(path))\n",
    "                exit(1)\n",
    "\n",
    "    def _download(self, fpath, link):\n",
    "        print(\"Downloading from {} to: '{}'\".format(link, fpath))\n",
    "        with open(fpath, \"wb\") as file:\n",
    "            # get request\n",
    "            response = get(link)\n",
    "            # write to file\n",
    "            file.write(response.content)\n",
    "\n",
    "    def _load_and_slice_data(self, rdir, infos):\n",
    "        self.X_train = np.zeros((0, self.length))\n",
    "        self.X_test = np.zeros((0, self.length))\n",
    "        #self.X_train = []\n",
    "        #self.X_test = []\n",
    "        self.y_train = []\n",
    "        self.y_test = []\n",
    "        #print(infos)\n",
    "        for idx, info in enumerate(infos):\n",
    "            # directory of this file\n",
    "            fdir = os.path.join(rdir, info[0], info[1])\n",
    "            self._mkdir(fdir)\n",
    "            fpath = os.path.join(fdir, info[2] + '.mat')\n",
    "            if not os.path.exists(fpath):\n",
    "                self._download(fpath, info[3].rstrip('\\n'))\n",
    "\n",
    "            mat_dict = loadmat(fpath)\n",
    "            key = list(filter(lambda x: 'DE_time' in x, mat_dict.keys()))[0]\n",
    "            time_series = mat_dict[key][:, 0]\n",
    "\n",
    "            idx_last = -(time_series.shape[0] % self.length)\n",
    "            clips = time_series[:idx_last].reshape(-1, self.length)\n",
    "            n = clips.shape[0]\n",
    "            n_split = int(3 * n / 4)\n",
    "#             temp_X_train = np.zeros((0, self.length))\n",
    "#             temp_X_test = np.zeros((0, self.length))\n",
    "            self.X_train = np.vstack((self.X_train, clips[:n_split]))\n",
    "            self.X_test = np.vstack((self.X_test, clips[n_split:]))\n",
    "            #self.X_train.append(clips[:n_split])\n",
    "            #self.X_test.append(clips[n_split:300])\n",
    "#             self.X_train.append(temp_X_train)\n",
    "#             self.X_test.append(temp_X_test)\n",
    "            self.y_train += [idx] * n_split\n",
    "            self.y_test += [idx] * (clips.shape[0] - n_split)\n",
    "        #self.X_train = np.dstack(self.X_train)\n",
    "        #self.y_train = np.dstack(self.y_train)\n",
    "        #self.X_test = np.dstack(self.X_test)\n",
    "        #self.y_test = np.dstack(self.y_test)\n",
    "\n",
    "    def _shuffle(self):\n",
    "        # shuffle training samples\n",
    "        index = list(range(self.X_train.shape[0]))\n",
    "        random.Random(0).shuffle(index)\n",
    "        self.X_train = self.X_train[index]\n",
    "        self.y_train = tuple(self.y_train[i] for i in index)\n",
    "\n",
    "        # shuffle test samples\n",
    "        index = list(range(self.X_test.shape[0]))\n",
    "        random.Random(0).shuffle(index)\n",
    "        self.X_test = self.X_test[index]\n",
    "        self.y_test = tuple(self.y_test[i] for i in index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always run from G:\\Machine Learning\\git_code\\time-series\\\n",
    "# Dataset is at G:\\Machine Learning\\git_code\\time-series\\DataSetCWRU\n",
    "data = CWRU(\"12DriveEndFault\", \"1772\", 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.X_train = np.array(data.X_train)\n",
    "data.y_train = np.array(data.y_train)\n",
    "data.X_test = np.array(data.X_test)\n",
    "data.y_test = np.array(data.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.X_train.shape, data.y_train.shape, data.X_test.shape, data.y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.y_train = data.y_train.reshape(data.y_train.shape[1], data.y_train.shape[0], 1)\n",
    "#data.y_test = data.y_test.reshape(data.y_test.shape[1], data.y_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.X_train = data.X_train.reshape(data.X_train.shape[0], data.X_train.shape[1], 1)\n",
    "data.X_test = data.X_test.reshape(data.X_test.shape[0], data.X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.X_train.shape, data.y_train.shape, data.X_test.shape, data.y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.X_train = pd.DataFrame(data.X_train)\n",
    "# data.y_train = pd.DataFrame(data.y_train)\n",
    "# data.X_test = pd.DataFrame(data.X_test)\n",
    "# data.y_test = pd.DataFrame(data.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.y_train.iloc[36].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        vy = y.iloc[i + time_steps].values\n",
    "        ys.append(vy)\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME_STEPS = 12\n",
    "\n",
    "# # reshape to [samples, time_steps, n_features]\n",
    "\n",
    "# Xtrain, ytrain = create_dataset(\n",
    "#   data.X_train,\n",
    "#   data.y_train,\n",
    "#   TIME_STEPS\n",
    "# )\n",
    "\n",
    "# Xtest, ytest = create_dataset(\n",
    "#   data.X_test,\n",
    "#   data.y_test,\n",
    "#   TIME_STEPS\n",
    "# )\n",
    "\n",
    "# print(Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.X_train = data.X_train.reshape(data.X_train.shape[0], data.X_train.shape[1], 1)\n",
    "# data.X_test = data.X_test.reshape(data.X_test.shape[0], data.X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    " \n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded\n",
    " \n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(data, prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = data.X_train, data.y_train\n",
    "    print(trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = data.X_test, data.y_test\n",
    "    print(testX.shape, testy.shape)\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy\n",
    " \n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 1, 10, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(n_timesteps,n_features), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64, input_shape=(n_timesteps,n_features), return_sequences=True))\n",
    "    model.add(LSTM(32, input_shape=(n_timesteps,n_features)))\n",
    "    #model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    history = model.fit(trainX, trainy, epochs=epochs, validation_split=0.1, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=1)\n",
    "    yhat = model.predict(testX, verbose=0)\n",
    "    return yhat, history, accuracy\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "# run an experiment\n",
    "def run_experiment(repeats=1):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset(data)\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        yhat, history, score = evaluate_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "    return yhat, history\n",
    "\n",
    "# run the experiment\n",
    "yhat, history = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yh = [np.argmax(y) for y in yhat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(yh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#yhat = model.predict(data.X_test, verbose=0)\n",
    "ytest = to_categorical(data.y_test)\n",
    "yhat.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(yh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytestc = np.argmax(ytest, axis=1)\n",
    "#yhatc = np.argmax(yhat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ytest = data.y_test.reshape(data.y_test.shape[0], 1)\n",
    "rmse = np.sqrt(mean_squared_error(ytestc, yh))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## time steps, every step is one hour (you can easily convert the time step to the actual time index)\n",
    "## for a demonstration purpose, I only compare the predictions in 200 hours. \n",
    "import seaborn as sns\n",
    "aa=[x for x in range(len(ytest))]\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(ytestc, label='actual')\n",
    "plt.hist(yh, label='prediction')\n",
    "plt.ylabel('Faults', size=15)\n",
    "plt.xlabel('Time', size=15)\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
